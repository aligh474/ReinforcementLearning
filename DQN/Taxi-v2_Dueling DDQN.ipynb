{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, LSTM, Lambda,Add,Embedding,Reshape\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3QNAgent:\n",
    "    \"\"\" Agent Class (Network) for DDQN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim,gamma=0.95,epsilon=1.0,epsilon_min=0.01\n",
    "                ,epsilon_decay=0.9999, batch_size=32,ddqn=True,Soft_Update=False,dueling=True):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        \n",
    "        self.EPISODES = 2000\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "        self.gamma = gamma    # discount rate\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_min = epsilon_min # minimum exploration probability\n",
    "        self.epsilon_decay = epsilon_decay # exponential decay rate for exploration prob\n",
    "        self.batch_size = batch_size \n",
    "        self.train_start = 1000\n",
    "\n",
    "        # defining model parameters\n",
    "        self.ddqn = ddqn # use doudle deep q network\n",
    "        self.Soft_Update = Soft_Update # use soft parameter update\n",
    "        self.dueling = dueling # use dealing netowrk\n",
    "\n",
    "        self.TAU = 0.1 # target network soft update hyperparameter\n",
    "        \n",
    "        # Initialize Deep Q-Network\n",
    "        t = (self.state_dim,)\n",
    "        self.model = self.network(t,self.action_dim,self.dueling)\n",
    "        # Build target Q-Network\n",
    "        self.target_model = self.network(t,self.action_dim,self.dueling)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def huber_loss(self, y_true, y_pred):\n",
    "        return K.mean(K.sqrt(1 + K.square(y_pred - y_true)) - 1, axis=-1)\n",
    "\n",
    "    def network(self, input_shape, action_space, dueling):\n",
    "        \"\"\" Build Deep Q-Network\n",
    "        \"\"\"\n",
    "        X_input = Input(shape=input_shape)\n",
    "        X = Embedding(500, 10, input_length=1)(X_input)\n",
    "        X = Flatten()(X)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "        X = Dense(128, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        if dueling:\n",
    "            state_value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "            #Q = V + A - mean(A)\n",
    "            #V = s[:, 0]\n",
    "            state_value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(action_space,))(state_value)\n",
    "\n",
    "            action_advantage = Dense(action_space, kernel_initializer='he_uniform')(X)\n",
    "            #Q = V + A - mean(A)\n",
    "            #A = a[:, :]\n",
    "            # mean(A) = K.mean(a[:, :], keepdims=True)\n",
    "            action_advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(action_space,))(action_advantage)\n",
    "\n",
    "            X = Add()([state_value, action_advantage])\n",
    "        else:\n",
    "            # Output Layer with # of actions: 2 nodes (left, right)\n",
    "            X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        model = Model(inputs = X_input, outputs = X, name='D3QN_model')\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "        model.summary()\n",
    "        return model\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        if not self.Soft_Update:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            return\n",
    "        else :\n",
    "            q_model_W = self.model.get_weights()\n",
    "            target_model_W = self.target_model.get_weights()\n",
    "            index = 0\n",
    "            for q_weight, target_weight in zip(q_model_W, target_model_W):\n",
    "                target_weight = target_weight * (1-self.TAU) + q_weight * self.TAU\n",
    "                target_model_W[index] = target_weight\n",
    "                index += 1\n",
    "            self.target_model.set_weights(target_model_W)\n",
    "            return\n",
    "\n",
    "    def choose_action(self,obs):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_dim) \n",
    "        else:\n",
    "            return np.argmax(self.model.predict(obs))\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, done = self.preprocess_mem(minibatch)\n",
    "        \n",
    "        target = self.model.predict(states)\n",
    "    \n",
    "        #double dqn: Q_target(s_next,max_a(Q(s,a)))\n",
    "        target_next = self.model.predict(next_states)\n",
    "        target_val = self.target_model.predict(next_states)\n",
    "        \n",
    "        for i in range(len(minibatch)):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                if self.ddqn: # Double - DQN\n",
    "                    # current Q Network selects the action\n",
    "                    # a'_max = argmax_a' Q(s', a')\n",
    "                    a = np.argmax(target_next[i])\n",
    "                    # target Q Network evaluates the action\n",
    "                    # Q_max = Q_target(s', a'_max)\n",
    "                    target[i][actions[i]] = rewards[i] + self.gamma * (target_val[i][a])   \n",
    "                else: # Standard - DQN\n",
    "                    # DQN chooses the max Q value among next actions\n",
    "                    # selection and evaluation of action is on the target Q Network\n",
    "                    # Q_max = max_a' Q_target(s', a')\n",
    "                    target[i][action[i]] = rewards[i] + self.gamma * (np.amax(target_next[i]))\n",
    "                    \n",
    "        self.model.fit(states, target, batch_size=self.batch_size, verbose=0)\n",
    "        \n",
    "        \n",
    "    def preprocess_mem(self,minibatch):\n",
    "        state = np.zeros((self.batch_size, self.state_dim))\n",
    "        next_state = np.zeros((self.batch_size, self.state_dim))\n",
    "        action, reward, done = [], [], []\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "        return state,action,reward,next_state,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"D3QN_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 10)        5000        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 10)           0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 256)          2816        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          32896       dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 64)           8256        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            65          dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 6)            390         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1)            0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 6)            0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 6)            0           lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 49,423\n",
      "Trainable params: 49,423\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"D3QN_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 10)        5000        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 10)           0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 256)          2816        flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 128)          32896       dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 64)           8256        dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            65          dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 6)            390         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 1)            0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 6)            0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 6)            0           lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 49,423\n",
      "Trainable params: 49,423\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env_name = 'Taxi-v2'\n",
    "env = gym.make(env_name)\n",
    "state_size = 1\n",
    "agent = D3QNAgent(state_size, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 200,0/1000, score: -686, e: 1.0,avg:-653.55 \n",
      "episode: 200,1/1000, score: -677, e: 1.0,avg:-656.7 \n",
      "episode: 200,2/1000, score: -803, e: 1.0,avg:-753.9 \n",
      "episode: 200,3/1000, score: -677, e: 1.0,avg:-645.0 \n",
      "episode: 200,4/1000, score: -830, e: 1.0,avg:-792.15 \n",
      "episode: 200,5/1000, score: -749, e: 0.98,avg:-709.8 \n",
      "episode: 200,6/1000, score: -830, e: 0.96,avg:-790.35 \n",
      "episode: 200,7/1000, score: -893, e: 0.94,avg:-851.1 \n",
      "episode: 42,8/1000, score: -75, e: 0.94,avg:-77.8 \n",
      "episode: 200,9/1000, score: -767, e: 0.92,avg:-730.5 \n",
      "episode: 200,10/1000, score: -668, e: 0.9,avg:-619.8 \n",
      "episode: 200,11/1000, score: -731, e: 0.88,avg:-711.15 \n",
      "episode: 200,12/1000, score: -605, e: 0.87,avg:-568.5 \n",
      "episode: 200,13/1000, score: -713, e: 0.85,avg:-675.6 \n",
      "episode: 200,14/1000, score: -641, e: 0.83,avg:-618.45 \n",
      "episode: 200,15/1000, score: -677, e: 0.82,avg:-631.5 \n",
      "episode: 111,16/1000, score: -261, e: 0.81,avg:-264.7 \n",
      "episode: 200,17/1000, score: -641, e: 0.79,avg:-622.05 \n",
      "episode: 200,18/1000, score: -713, e: 0.77,avg:-696.3 \n",
      "episode: 200,19/1000, score: -713, e: 0.76,avg:-676.05 \n",
      "episode: 200,20/1000, score: -650, e: 0.74,avg:-609.45 \n",
      "episode: 200,21/1000, score: -668, e: 0.73,avg:-635.1 \n",
      "episode: 200,22/1000, score: -596, e: 0.72,avg:-565.35 \n",
      "episode: 200,23/1000, score: -740, e: 0.7,avg:-694.05 \n",
      "episode: 200,24/1000, score: -470, e: 0.69,avg:-440.25 \n",
      "episode: 200,25/1000, score: -605, e: 0.67,avg:-595.05 \n",
      "episode: 200,26/1000, score: -533, e: 0.66,avg:-504.15 \n",
      "episode: 200,27/1000, score: -632, e: 0.65,avg:-584.25 \n",
      "episode: 200,28/1000, score: -614, e: 0.63,avg:-584.25 \n",
      "episode: 200,29/1000, score: -587, e: 0.62,avg:-567.6 \n",
      "episode: 200,30/1000, score: -614, e: 0.61,avg:-591.45 \n",
      "episode: 139,31/1000, score: -334, e: 0.6,avg:-330.5 \n",
      "episode: 200,32/1000, score: -524, e: 0.59,avg:-498.75 \n",
      "episode: 200,33/1000, score: -434, e: 0.58,avg:-402.45 \n",
      "episode: 200,34/1000, score: -605, e: 0.57,avg:-579.3 \n",
      "episode: 200,35/1000, score: -542, e: 0.55,avg:-508.2 \n",
      "episode: 200,36/1000, score: -533, e: 0.54,avg:-521.25 \n",
      "episode: 200,37/1000, score: -551, e: 0.53,avg:-520.35 \n",
      "episode: 200,38/1000, score: -434, e: 0.52,avg:-414.6 \n",
      "episode: 200,39/1000, score: -479, e: 0.51,avg:-469.05 \n",
      "episode: 200,40/1000, score: -542, e: 0.5,avg:-507.3 \n",
      "episode: 200,41/1000, score: -353, e: 0.49,avg:-332.25 \n",
      "episode: 178,42/1000, score: -454, e: 0.48,avg:-432.95 \n",
      "episode: 200,43/1000, score: -506, e: 0.47,avg:-483.45 \n",
      "episode: 200,44/1000, score: -407, e: 0.46,avg:-391.2 \n",
      "episode: 200,45/1000, score: -434, e: 0.46,avg:-418.65 \n",
      "episode: 200,46/1000, score: -461, e: 0.45,avg:-447.45 \n",
      "episode: 200,47/1000, score: -479, e: 0.44,avg:-433.95 \n",
      "episode: 200,48/1000, score: -479, e: 0.43,avg:-456.45 \n",
      "episode: 78,49/1000, score: -129, e: 0.43,avg:-136.75 \n",
      "episode: 200,50/1000, score: -389, e: 0.42,avg:-373.65 \n",
      "episode: 200,51/1000, score: -461, e: 0.41,avg:-443.4 \n",
      "episode: 200,52/1000, score: -362, e: 0.4,avg:-340.8 \n",
      "episode: 164,53/1000, score: -305, e: 0.39,avg:-304.2 \n",
      "episode: 200,54/1000, score: -425, e: 0.39,avg:-408.75 \n",
      "episode: 200,55/1000, score: -425, e: 0.38,avg:-415.5 \n",
      "episode: 200,56/1000, score: -308, e: 0.37,avg:-289.95 \n",
      "episode: 200,57/1000, score: -317, e: 0.36,avg:-303.45 \n",
      "episode: 200,58/1000, score: -425, e: 0.36,avg:-412.8 \n",
      "episode: 200,59/1000, score: -353, e: 0.35,avg:-322.8 \n",
      "episode: 200,60/1000, score: -362, e: 0.34,avg:-348.9 \n",
      "episode: 200,61/1000, score: -335, e: 0.34,avg:-325.5 \n",
      "episode: 137,62/1000, score: -206, e: 0.33,avg:-211.95 \n",
      "episode: 200,63/1000, score: -407, e: 0.32,avg:-383.55 \n",
      "episode: 200,64/1000, score: -452, e: 0.32,avg:-435.75 \n",
      "episode: 200,65/1000, score: -335, e: 0.31,avg:-322.8 \n",
      "episode: 51,66/1000, score: -75, e: 0.31,avg:-58.9 \n",
      "episode: 200,67/1000, score: -398, e: 0.3,avg:-383.55 \n",
      "episode: 200,68/1000, score: -353, e: 0.3,avg:-340.35 \n",
      "episode: 200,69/1000, score: -362, e: 0.29,avg:-345.3 \n",
      "episode: 200,70/1000, score: -389, e: 0.29,avg:-372.3 \n",
      "episode: 200,71/1000, score: -335, e: 0.28,avg:-320.1 \n",
      "episode: 193,72/1000, score: -307, e: 0.28,avg:-311.15 \n",
      "episode: 200,73/1000, score: -344, e: 0.27,avg:-334.5 \n",
      "episode: 200,74/1000, score: -380, e: 0.26,avg:-349.35 \n",
      "episode: 200,75/1000, score: -308, e: 0.26,avg:-298.5 \n",
      "episode: 200,76/1000, score: -326, e: 0.25,avg:-316.5 \n",
      "episode: 200,77/1000, score: -371, e: 0.25,avg:-357.45 \n",
      "episode: 200,78/1000, score: -326, e: 0.24,avg:-297.6 \n",
      "episode: 200,79/1000, score: -335, e: 0.24,avg:-303.0 \n",
      "episode: 200,80/1000, score: -299, e: 0.23,avg:-282.3 \n",
      "episode: 200,81/1000, score: -326, e: 0.23,avg:-307.95 \n",
      "episode: 200,82/1000, score: -371, e: 0.23,avg:-342.6 \n",
      "episode: 200,83/1000, score: -317, e: 0.22,avg:-301.65 \n",
      "episode: 106,84/1000, score: -193, e: 0.22,avg:-198.05 \n",
      "episode: 200,85/1000, score: -308, e: 0.21,avg:-298.05 \n",
      "episode: 200,86/1000, score: -326, e: 0.21,avg:-316.5 \n",
      "episode: 200,87/1000, score: -371, e: 0.21,avg:-343.05 \n",
      "episode: 200,88/1000, score: -317, e: 0.2,avg:-305.7 \n",
      "episode: 200,89/1000, score: -326, e: 0.2,avg:-314.25 \n",
      "episode: 200,90/1000, score: -299, e: 0.19,avg:-284.1 \n",
      "episode: 200,91/1000, score: -326, e: 0.19,avg:-305.25 \n",
      "episode: 200,92/1000, score: -308, e: 0.19,avg:-287.7 \n",
      "episode: 200,93/1000, score: -272, e: 0.18,avg:-262.5 \n",
      "episode: 200,94/1000, score: -254, e: 0.18,avg:-244.5 \n",
      "episode: 200,95/1000, score: -344, e: 0.18,avg:-334.05 \n",
      "episode: 200,96/1000, score: -281, e: 0.17,avg:-271.5 \n",
      "episode: 200,97/1000, score: -263, e: 0.17,avg:-249.45 \n",
      "episode: 200,98/1000, score: -227, e: 0.17,avg:-217.5 \n",
      "episode: 200,99/1000, score: -245, e: 0.16,avg:-235.5 \n",
      "episode: 200,100/1000, score: -254, e: 0.16,avg:-244.5 \n",
      "episode: 200,101/1000, score: -281, e: 0.16,avg:-263.85 \n",
      "episode: 200,102/1000, score: -290, e: 0.15,avg:-268.35 \n",
      "episode: 200,103/1000, score: -326, e: 0.15,avg:-316.5 \n",
      "episode: 200,104/1000, score: -317, e: 0.15,avg:-305.7 \n",
      "episode: 200,105/1000, score: -263, e: 0.14,avg:-247.2 \n",
      "episode: 200,106/1000, score: -272, e: 0.14,avg:-262.05 \n",
      "episode: 200,107/1000, score: -281, e: 0.14,avg:-265.65 \n",
      "episode: 144,108/1000, score: -132, e: 0.14,avg:-142.45 \n",
      "episode: 200,109/1000, score: -245, e: 0.13,avg:-235.5 \n",
      "episode: 200,110/1000, score: -272, e: 0.13,avg:-259.8 \n",
      "episode: 200,111/1000, score: -236, e: 0.13,avg:-221.55 \n",
      "episode: 200,112/1000, score: -254, e: 0.13,avg:-238.65 \n",
      "episode: 200,113/1000, score: -245, e: 0.12,avg:-235.5 \n",
      "episode: 200,114/1000, score: -290, e: 0.12,avg:-275.55 \n",
      "episode: 200,115/1000, score: -272, e: 0.12,avg:-262.5 \n",
      "episode: 200,116/1000, score: -236, e: 0.12,avg:-226.5 \n",
      "episode: 200,117/1000, score: -263, e: 0.11,avg:-253.5 \n",
      "episode: 200,118/1000, score: -245, e: 0.11,avg:-235.5 \n",
      "episode: 200,119/1000, score: -515, e: 0.11,avg:-505.5 \n",
      "episode: 200,120/1000, score: -290, e: 0.11,avg:-271.95 \n",
      "episode: 200,121/1000, score: -254, e: 0.1,avg:-240.45 \n",
      "episode: 200,122/1000, score: -245, e: 0.1,avg:-235.5 \n",
      "episode: 137,123/1000, score: -998, e: 0.1,avg:-949.05 \n",
      "episode: 200,124/1000, score: -290, e: 0.099,avg:-280.5 \n",
      "episode: 200,125/1000, score: -254, e: 0.097,avg:-244.5 \n",
      "episode: 200,126/1000, score: -245, e: 0.096,avg:-235.5 \n",
      "episode: 200,127/1000, score: -209, e: 0.094,avg:-199.5 \n",
      "episode: 200,128/1000, score: -272, e: 0.092,avg:-262.5 \n",
      "episode: 200,129/1000, score: -272, e: 0.09,avg:-248.55 \n",
      "episode: 200,130/1000, score: -245, e: 0.088,avg:-227.85 \n",
      "episode: 200,131/1000, score: -263, e: 0.086,avg:-253.5 \n",
      "episode: 200,132/1000, score: -227, e: 0.085,avg:-215.7 \n",
      "episode: 200,133/1000, score: -308, e: 0.083,avg:-285.0 \n",
      "episode: 200,134/1000, score: -281, e: 0.081,avg:-266.1 \n",
      "episode: 200,135/1000, score: -227, e: 0.08,avg:-217.5 \n",
      "episode: 200,136/1000, score: -218, e: 0.078,avg:-208.5 \n",
      "episode: 200,137/1000, score: -263, e: 0.077,avg:-249.9 \n",
      "episode: 200,138/1000, score: -236, e: 0.075,avg:-226.5 \n",
      "episode: 106,139/1000, score: -103, e: 0.074,avg:-104.9 \n",
      "episode: 200,140/1000, score: -245, e: 0.073,avg:-235.5 \n",
      "episode: 200,141/1000, score: -236, e: 0.071,avg:-223.8 \n",
      "episode: 200,142/1000, score: -272, e: 0.07,avg:-257.1 \n",
      "episode: 200,143/1000, score: -272, e: 0.069,avg:-250.8 \n",
      "episode: 200,144/1000, score: -218, e: 0.067,avg:-208.5 \n",
      "episode: 200,145/1000, score: -254, e: 0.066,avg:-244.5 \n",
      "episode: 200,146/1000, score: -245, e: 0.065,avg:-235.5 \n",
      "episode: 200,147/1000, score: -227, e: 0.063,avg:-217.5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 200,148/1000, score: -227, e: 0.062,avg:-217.5 \n",
      "episode: 200,149/1000, score: -254, e: 0.061,avg:-235.95 \n",
      "episode: 200,150/1000, score: -254, e: 0.06,avg:-239.55 \n",
      "episode: 59,151/1000, score: -47, e: 0.059,avg:-49.35 \n",
      "episode: 200,152/1000, score: -227, e: 0.058,avg:-217.5 \n",
      "episode: 200,153/1000, score: -236, e: 0.057,avg:-221.55 \n",
      "episode: 200,154/1000, score: -227, e: 0.056,avg:-209.85 \n",
      "episode: 54,155/1000, score: -42, e: 0.056,avg:-52.45 \n",
      "episode: 200,156/1000, score: -200, e: 0.054,avg:-190.5 \n",
      "episode: 200,157/1000, score: -227, e: 0.053,avg:-209.85 \n",
      "episode: 200,158/1000, score: -218, e: 0.052,avg:-208.5 \n",
      "episode: 200,159/1000, score: -218, e: 0.051,avg:-208.5 \n",
      "episode: 200,160/1000, score: -227, e: 0.05,avg:-215.25 \n",
      "episode: 200,161/1000, score: -245, e: 0.049,avg:-235.5 \n",
      "episode: 200,162/1000, score: -218, e: 0.048,avg:-208.5 \n",
      "episode: 200,163/1000, score: -245, e: 0.047,avg:-235.5 \n",
      "episode: 200,164/1000, score: -200, e: 0.046,avg:-190.5 \n",
      "episode: 200,165/1000, score: -227, e: 0.045,avg:-217.5 \n",
      "episode: 200,166/1000, score: -209, e: 0.045,avg:-199.5 \n",
      "episode: 200,167/1000, score: -236, e: 0.044,avg:-226.5 \n",
      "episode: 200,168/1000, score: -209, e: 0.043,avg:-199.5 \n",
      "episode: 200,169/1000, score: -227, e: 0.042,avg:-217.5 \n",
      "episode: 200,170/1000, score: -218, e: 0.041,avg:-202.65 \n",
      "episode: 200,171/1000, score: -227, e: 0.04,avg:-210.3 \n",
      "episode: 200,172/1000, score: -227, e: 0.04,avg:-217.5 \n",
      "episode: 200,173/1000, score: -218, e: 0.039,avg:-208.5 \n",
      "episode: 200,174/1000, score: -218, e: 0.038,avg:-208.5 \n",
      "episode: 200,175/1000, score: -236, e: 0.037,avg:-226.5 \n",
      "episode: 155,176/1000, score: -791, e: 0.037,avg:-732.6 \n",
      "episode: 200,177/1000, score: -227, e: 0.036,avg:-217.5 \n",
      "episode: 200,178/1000, score: -218, e: 0.035,avg:-208.5 \n",
      "episode: 200,179/1000, score: -254, e: 0.035,avg:-236.85 \n",
      "episode: 200,180/1000, score: -200, e: 0.034,avg:-190.5 \n",
      "episode: 200,181/1000, score: -227, e: 0.033,avg:-213.0 \n",
      "episode: 200,182/1000, score: -209, e: 0.033,avg:-199.5 \n",
      "episode: 200,183/1000, score: -209, e: 0.032,avg:-195.45 \n",
      "episode: 200,184/1000, score: -209, e: 0.031,avg:-199.5 \n",
      "episode: 200,185/1000, score: -209, e: 0.031,avg:-195.0 \n",
      "episode: 200,186/1000, score: -209, e: 0.03,avg:-199.5 \n",
      "episode: 200,187/1000, score: -209, e: 0.029,avg:-199.5 \n",
      "episode: 200,188/1000, score: -218, e: 0.029,avg:-208.5 \n",
      "episode: 200,189/1000, score: -200, e: 0.028,avg:-190.5 \n",
      "episode: 200,190/1000, score: -227, e: 0.028,avg:-217.5 \n",
      "episode: 200,191/1000, score: -218, e: 0.027,avg:-208.5 \n",
      "episode: 200,192/1000, score: -200, e: 0.027,avg:-190.5 \n",
      "episode: 200,193/1000, score: -245, e: 0.026,avg:-227.85 \n",
      "episode: 200,194/1000, score: -227, e: 0.026,avg:-217.5 \n",
      "episode: 200,195/1000, score: -200, e: 0.025,avg:-190.5 \n",
      "episode: 200,196/1000, score: -200, e: 0.025,avg:-190.5 \n",
      "episode: 200,197/1000, score: -200, e: 0.024,avg:-190.5 \n",
      "episode: 200,198/1000, score: -227, e: 0.024,avg:-217.5 \n",
      "episode: 200,199/1000, score: -227, e: 0.023,avg:-217.5 \n",
      "episode: 200,200/1000, score: -218, e: 0.023,avg:-208.5 \n",
      "episode: 200,201/1000, score: -227, e: 0.022,avg:-217.5 \n",
      "episode: 200,202/1000, score: -218, e: 0.022,avg:-208.5 \n",
      "episode: 200,203/1000, score: -200, e: 0.021,avg:-190.5 \n",
      "episode: 200,204/1000, score: -209, e: 0.021,avg:-199.5 \n",
      "episode: 200,205/1000, score: -227, e: 0.021,avg:-217.5 \n",
      "episode: 200,206/1000, score: -218, e: 0.02,avg:-208.5 \n",
      "episode: 200,207/1000, score: -209, e: 0.02,avg:-199.5 \n",
      "episode: 200,208/1000, score: -209, e: 0.019,avg:-199.5 \n",
      "episode: 200,209/1000, score: -200, e: 0.019,avg:-190.5 \n",
      "episode: 200,210/1000, score: -200, e: 0.019,avg:-190.5 \n",
      "episode: 200,211/1000, score: -209, e: 0.018,avg:-199.5 \n",
      "episode: 200,212/1000, score: -200, e: 0.018,avg:-190.5 \n",
      "episode: 200,213/1000, score: -218, e: 0.017,avg:-208.5 \n",
      "episode: 200,214/1000, score: -200, e: 0.017,avg:-190.5 \n",
      "episode: 200,215/1000, score: -200, e: 0.017,avg:-190.5 \n",
      "episode: 200,216/1000, score: -200, e: 0.016,avg:-190.5 \n",
      "episode: 200,217/1000, score: -218, e: 0.016,avg:-208.5 \n",
      "episode: 200,218/1000, score: -209, e: 0.016,avg:-199.5 \n",
      "episode: 200,219/1000, score: -200, e: 0.016,avg:-190.5 \n",
      "episode: 200,220/1000, score: -200, e: 0.015,avg:-190.5 \n",
      "episode: 200,221/1000, score: -200, e: 0.015,avg:-190.5 \n",
      "episode: 200,222/1000, score: -200, e: 0.015,avg:-190.5 \n",
      "episode: 200,223/1000, score: -200, e: 0.014,avg:-190.5 \n",
      "episode: 200,224/1000, score: -209, e: 0.014,avg:-199.5 \n",
      "episode: 200,225/1000, score: -209, e: 0.014,avg:-199.5 \n",
      "episode: 200,226/1000, score: -209, e: 0.013,avg:-199.5 \n",
      "episode: 200,227/1000, score: -218, e: 0.013,avg:-208.5 \n",
      "episode: 200,228/1000, score: -218, e: 0.013,avg:-208.5 \n",
      "episode: 200,229/1000, score: -209, e: 0.013,avg:-199.5 \n",
      "episode: 200,230/1000, score: -623, e: 0.012,avg:-543.75 \n",
      "episode: 200,231/1000, score: -200, e: 0.012,avg:-190.5 \n",
      "episode: 200,232/1000, score: -200, e: 0.012,avg:-190.5 \n",
      "episode: 200,233/1000, score: -200, e: 0.012,avg:-190.5 \n",
      "episode: 200,234/1000, score: -209, e: 0.011,avg:-199.5 \n",
      "episode: 200,235/1000, score: -209, e: 0.011,avg:-199.5 \n",
      "episode: 200,236/1000, score: -209, e: 0.011,avg:-191.85 \n",
      "episode: 200,237/1000, score: -362, e: 0.011,avg:-352.5 \n",
      "episode: 200,238/1000, score: -200, e: 0.011,avg:-190.5 \n",
      "episode: 200,239/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,240/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,241/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,242/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,243/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,244/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,245/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,246/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,247/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,248/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,249/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,250/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,251/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,252/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,253/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 172,254/1000, score: -151, e: 0.01,avg:-161.45 \n",
      "episode: 200,255/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,256/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,257/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,258/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,259/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,260/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,261/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,262/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,263/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,264/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,265/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,266/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,267/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,268/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,269/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,270/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,271/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,272/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,273/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,274/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,275/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,276/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,277/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,278/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,279/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,280/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,281/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,282/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,283/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,284/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,285/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,286/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,287/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,288/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,289/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,290/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,291/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,292/1000, score: -200, e: 0.01,avg:-190.5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 200,293/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,294/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,295/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,296/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,297/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,298/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,299/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,300/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,301/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,302/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,303/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,304/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,305/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,306/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,307/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,308/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,309/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,310/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,311/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,312/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,313/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,314/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,315/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,316/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,317/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,318/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,319/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,320/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,321/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,322/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,323/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,324/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,325/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,326/1000, score: -227, e: 0.01,avg:-217.5 \n",
      "episode: 200,327/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,328/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,329/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,330/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,331/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,332/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,333/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,334/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,335/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,336/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,337/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,338/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,339/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,340/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,341/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,342/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,343/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,344/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,345/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,346/1000, score: -227, e: 0.01,avg:-209.85 \n",
      "episode: 200,347/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,348/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,349/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,350/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,351/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,352/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,353/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,354/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,355/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,356/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,357/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,358/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,359/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,360/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,361/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,362/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,363/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,364/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,365/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,366/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,367/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,368/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,369/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,370/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,371/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,372/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,373/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,374/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,375/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,376/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,377/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,378/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,379/1000, score: -218, e: 0.01,avg:-206.25 \n",
      "episode: 200,380/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,381/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,382/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,383/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,384/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,385/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,386/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,387/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,388/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,389/1000, score: -227, e: 0.01,avg:-211.2 \n",
      "episode: 200,390/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,391/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,392/1000, score: -236, e: 0.01,avg:-221.1 \n",
      "episode: 200,393/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,394/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,395/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,396/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,397/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,398/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,399/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,400/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,401/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,402/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,403/1000, score: -209, e: 0.01,avg:-195.0 \n",
      "episode: 200,404/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,405/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,406/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,407/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,408/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,409/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,410/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,411/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,412/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,413/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,414/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,415/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,416/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,417/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,418/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,419/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,420/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,421/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,422/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,423/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,424/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,425/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,426/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,427/1000, score: -209, e: 0.01,avg:-199.5 \n",
      "episode: 200,428/1000, score: -218, e: 0.01,avg:-208.5 \n",
      "episode: 200,429/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,430/1000, score: -200, e: 0.01,avg:-190.5 \n",
      "episode: 200,431/1000, score: -209, e: 0.01,avg:-199.5 \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "EPISODES = 1000\n",
    "scores=[]\n",
    "average =[]\n",
    "update =0\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, 1])\n",
    "    done = False\n",
    "    i = 0\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 1])\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        scores.append(total_reward)\n",
    "        i += 1\n",
    "        update += 1\n",
    "        if done:\n",
    "            print(\"episode: {},{}/{}, score: {}, e: {:.2},avg:{} \".format(i,e, EPISODES, total_reward, agent.epsilon,np.mean(scores[-20:])))\n",
    "        \n",
    "        if update % 100 == 0:\n",
    "            agent.update_target_model()\n",
    "      \n",
    "          \n",
    "        agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
