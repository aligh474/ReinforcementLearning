{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer:\n",
    "    def __init__(self, env, n_components=500):\n",
    "        observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(observation_examples)\n",
    "\n",
    "        featurizer = FeatureUnion([\n",
    "                (\"rbf1\", RBFSampler(gamma=5.0, n_components=n_components)),\n",
    "                (\"rbf2\", RBFSampler(gamma=2.0, n_components=n_components)),\n",
    "                (\"rbf3\", RBFSampler(gamma=1.0, n_components=n_components)),\n",
    "                (\"rbf4\", RBFSampler(gamma=0.5, n_components=n_components))\n",
    "                ])\n",
    "        example_features = featurizer.fit_transform(scaler.transform(observation_examples))\n",
    "\n",
    "        self.dimensions = example_features.shape[1]\n",
    "        self.scaler = scaler\n",
    "        self.featurizer = featurizer\n",
    "\n",
    "    def transform(self, observations):\n",
    "        scaled = self.scaler.transform(observations)\n",
    "        return self.featurizer.transform(scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def __init__(self, D):\n",
    "        self.w = np.random.randn(D) / np.sqrt(D)\n",
    "\n",
    "    def partial_fit(self, input_, target, eligibility, lr=1e-2):\n",
    "        self.w += lr*(target - input_.dot(self.w))*eligibility\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return X.dot(self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model:\n",
    "    def __init__(self, env, feature_transformer, learning_rate):\n",
    "        self.env = env\n",
    "        self.models = []\n",
    "        self.feature_transformer = feature_transformer\n",
    "        self.eligibilities = np.zeros((env.action_space.n, feature_transformer.dimensions))\n",
    "        for i in range(env.action_space.n):\n",
    "            model = BaseModel(feature_transformer.dimensions)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, s):\n",
    "        X = self.feature_transformer.transform([s])\n",
    "        result = np.stack([m.predict(X) for m in self.models]).T\n",
    "        return result\n",
    "\n",
    "    def update(self, s, a, G, gamma, lambda_):\n",
    "        X = self.feature_transformer.transform([s])\n",
    "        self.eligibilities *= gamma*lambda_\n",
    "        self.eligibilities[a] += X[0]\n",
    "        self.models[a].partial_fit(X[0], G, self.eligibilities[a])\n",
    "\n",
    "    def sample_action(self, s, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.predict(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one(model, env, eps, gamma, lambda_):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    # while not done and iters < 200:\n",
    "    while not done and iters < 10000:\n",
    "        action = model.sample_action(observation, eps)\n",
    "        prev_observation = observation\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # update the model\n",
    "        Q = model.predict(observation)\n",
    "        G = reward + gamma*np.max(Q[0])\n",
    "        model.update(prev_observation, action, G, gamma, lambda_)\n",
    "\n",
    "        totalreward += reward\n",
    "        iters += 1\n",
    "\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-03-31 20:00:15,825] Making new env: MountainCar-v0\n",
      "/home/ali/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward: -200.0\n",
      "episode: 100 total reward: -112.0\n",
      "episode: 200 total reward: -144.0\n",
      "episode: 300 total reward: -91.0\n",
      "episode: 400 total reward: -90.0\n",
      "episode: 500 total reward: -132.0\n",
      "episode: 600 total reward: -104.0\n",
      "episode: 700 total reward: -132.0\n",
      "episode: 800 total reward: -133.0\n",
      "episode: 900 total reward: -135.0\n",
      "episode: 1000 total reward: -158.0\n",
      "episode: 1100 total reward: -136.0\n",
      "episode: 1200 total reward: -146.0\n",
      "episode: 1300 total reward: -139.0\n",
      "episode: 1400 total reward: -93.0\n",
      "episode: 1500 total reward: -154.0\n",
      "episode: 1600 total reward: -200.0\n",
      "episode: 1700 total reward: -141.0\n",
      "episode: 1800 total reward: -141.0\n",
      "episode: 1900 total reward: -200.0\n",
      "episode: 2000 total reward: -143.0\n",
      "episode: 2100 total reward: -143.0\n",
      "episode: 2200 total reward: -200.0\n",
      "episode: 2300 total reward: -200.0\n",
      "episode: 2400 total reward: -148.0\n",
      "episode: 2500 total reward: -200.0\n",
      "episode: 2600 total reward: -144.0\n",
      "episode: 2700 total reward: -146.0\n",
      "episode: 2800 total reward: -200.0\n",
      "episode: 2900 total reward: -143.0\n",
      "episode: 3000 total reward: -146.0\n",
      "episode: 3100 total reward: -145.0\n",
      "episode: 3200 total reward: -200.0\n",
      "episode: 3300 total reward: -200.0\n",
      "episode: 3400 total reward: -144.0\n",
      "episode: 3500 total reward: -144.0\n",
      "episode: 3600 total reward: -195.0\n",
      "episode: 3700 total reward: -144.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "ft = FeatureTransformer(env)\n",
    "model = Model(env, ft, \"constant\")\n",
    "N = 4000\n",
    "gamma = 0.99\n",
    "lambda_ = 0.7\n",
    "totalrewards = np.empty(N)\n",
    "for n in range(N):\n",
    "    eps = 0.1*(0.97**n)\n",
    "    totalreward = play_one(model, env, eps, gamma, lambda_)\n",
    "    totalrewards[n] = totalreward\n",
    "    if n%100 == 0:\n",
    "        print(\"episode:\", n, \"total reward:\", totalreward)\n",
    "    \n",
    "print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "print(\"total steps:\", -totalrewards.sum())\n",
    "\n",
    "plt.plot(totalrewards)\n",
    "plt.title(\"Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
