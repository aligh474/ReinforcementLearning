{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Rewards with the Action Value Function\n",
    "\n",
    "This experiment is very similar to the one about the state value function. I recommend that you read that first. \n",
    "\n",
    "The action-value function is a view of the expected return with respect to a given state and action choice. The action represents an extra dimension over and above the state-value function. The premise is the same, but this time you need to iterate over all actions as well as all states. The equation is also similar, with the extra addition of an action, $a$:\n",
    "\n",
    "$$ Q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi}[ G \\vert s, a ] = \\mathbb{E}_{\\pi}\\bigg[ \\sum^{T}_{k=0} \\gamma^k r_{k} \\vert s, a \\bigg] $$\n",
    "\n",
    "Let's run the same experiment (as the state-value function experiment) again to see what the differences are.\n",
    "\n",
    "## The Environment: A Simple Grid World\n",
    "\n",
    "The first lot of code is exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_position = 1 # The starting position\n",
    "cliff_position = 0 # The cliff position\n",
    "end_position = 5 # The terminating state position\n",
    "reward_goal_state = 5 # Reward for reaching goal\n",
    "reward_cliff = 0 # Reward for falling off cliff\n",
    "\n",
    "def reward(current_position) -> int:\n",
    "    if current_position <= cliff_position:\n",
    "        return reward_cliff\n",
    "    if current_position >= end_position:\n",
    "        return reward_goal_state\n",
    "    return 0\n",
    "\n",
    "def is_terminating(current_position) -> bool:\n",
    "    if current_position <= cliff_position:\n",
    "        return True\n",
    "    if current_position >= end_position:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "The agent is also exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy() -> int:\n",
    "    if np.random.random() >= 0.5:\n",
    "        return 1 # Right\n",
    "    else:\n",
    "        return -1 # Left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Experiment\n",
    "\n",
    "However, here's where it differs. First off, there's far more exploration to do, because we're not only iterating over states, but also actions. You'll need to run this for longer before it converges.\n",
    "\n",
    "Also, we're going to have to store both the states and the actions in the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-99ce64153a7f>:38: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f'{q:.2f}' for q in value_sum[:, 0] / n_hits[:, 0])\n",
      "<ipython-input-4-99ce64153a7f>:40: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f'{q:.2f}' for q in value_sum[:, 1] / n_hits[:, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Average reward: [nan, 5.00, 5.00, 5.00, 5.00, 5.00 ; nan, nan, nan, nan, nan, nan]\n",
      "[1000] Average reward: [0.00, 1.15, 2.30, 3.21, 4.03, 5.00 ; 0.00, 0.81, 1.70, 2.79, 4.01, 5.00]\n",
      "[2000] Average reward: [0.00, 1.07, 2.10, 2.97, 3.81, 5.00 ; 0.00, 0.75, 1.55, 2.50, 3.62, 5.00]\n",
      "[3000] Average reward: [0.00, 1.07, 2.10, 3.02, 3.93, 5.00 ; 0.00, 0.79, 1.60, 2.49, 3.59, 5.00]\n",
      "[4000] Average reward: [0.00, 1.04, 2.05, 2.96, 3.88, 5.00 ; 0.00, 0.93, 1.82, 2.76, 3.80, 5.00]\n",
      "[5000] Average reward: [0.00, 1.04, 2.06, 3.03, 3.98, 5.00 ; 0.00, 0.97, 1.88, 2.84, 3.84, 5.00]\n",
      "[6000] Average reward: [0.00, 1.04, 2.06, 3.01, 3.97, 5.00 ; 0.00, 0.97, 1.90, 2.86, 3.84, 5.00]\n",
      "[7000] Average reward: [0.00, 1.06, 2.08, 3.04, 4.02, 5.00 ; 0.00, 0.97, 1.90, 2.85, 3.84, 5.00]\n",
      "[8000] Average reward: [0.00, 1.05, 2.08, 3.03, 3.98, 5.00 ; 0.00, 0.96, 1.91, 2.89, 3.88, 5.00]\n",
      "[9000] Average reward: [0.00, 1.03, 2.03, 2.99, 3.96, 5.00 ; 0.00, 0.95, 1.89, 2.89, 3.89, 5.00]\n",
      "[10000] Average reward: [0.00, 1.00, 1.98, 2.93, 3.93, 5.00 ; 0.00, 0.96, 1.90, 2.90, 3.91, 5.00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Global buffers to perform averaging later\n",
    "# Second dimension is the actions\n",
    "value_sum = np.zeros((end_position + 1, 2))\n",
    "n_hits = np.zeros((end_position + 1, 2))\n",
    "\n",
    "# A helper function to map the actions to valid buffer indices\n",
    "def action_value_mapping(x): return 0 if x == -1 else 1\n",
    "\n",
    "\n",
    "n_iter = 10001\n",
    "for i in range(n_iter):\n",
    "    position_history = [] # A log of positions in this episode\n",
    "    current_position = starting_position # Reset\n",
    "    current_action = strategy()\n",
    "    while True:\n",
    "        # Append position to log\n",
    "        position_history.append((current_position, current_action))\n",
    "\n",
    "        if is_terminating(current_position):\n",
    "            break\n",
    "        \n",
    "        # Update current position according to strategy\n",
    "        current_position += strategy()\n",
    "\n",
    "    # Now the episode has finished, what was the reward?\n",
    "    current_reward = reward(current_position)\n",
    "    \n",
    "    # Now add the reward to the buffers that allow you to calculate the average\n",
    "    for pos, act in position_history:\n",
    "        value_sum[pos, action_value_mapping(act)] += current_reward\n",
    "        n_hits[pos, action_value_mapping(act)] += 1\n",
    "        \n",
    "    # Now calculate the average for this episode and print\n",
    "    expect_return_0 = ', '.join(\n",
    "        f'{q:.2f}' for q in value_sum[:, 0] / n_hits[:, 0])\n",
    "    expect_return_1 = ', '.join(\n",
    "        f'{q:.2f}' for q in value_sum[:, 1] / n_hits[:, 1])\n",
    "    if i%1000==0:\n",
    "        print(\"[{}] Average reward: [{} ; {}]\".format(i, expect_return_0, expect_return_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-15a79e54a91f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.98\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.93\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.93\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.00\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.91\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.00\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "([0.00, 1.00, 1.98, 2.93, 3.93, 5.00]+ [0.00, 0.96, 1.90, 2.90, 3.91, 5.00])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
