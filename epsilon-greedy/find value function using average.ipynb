{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Rewards with the State Value Function\n",
    "\n",
    "The state-falue function is a view of the expected return with respect to each state. Below is the equation, where $G$ is the return, $s$ is the state, $\\gamma$ is the discount factor, and $r$ is the reward.\n",
    "\n",
    "$$ V_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[ G \\vert s] = \\mathbb{E}_{\\pi}\\bigg[ \\sum^{T}_{k=0} \\gamma^k r_{k} \\vert s \\bigg] $$\n",
    "\n",
    "You could estimate the expectation in a few ways, but the simplest is to simply average over al of the observed rewards. To investigate how this equation works, you can perform the calculation on a simple environment that is easy to validate.\n",
    "\n",
    "## The Environment: A Simple Grid World\n",
    "\n",
    "To demonstrate, let me use a rediculously simple grid-based environment. This consists of 5 squares, with a cliff on the left-hand side and a goal position on the right. Both are terminating states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_position = 1 # The starting position\n",
    "cliff_position = 0 # The cliff position\n",
    "end_position = 5 # The terminating state position\n",
    "reward_goal_state = 5 # Reward for reaching goal\n",
    "reward_cliff = 0 # Reward for falling off cliff\n",
    "\n",
    "def reward(current_position) -> int:\n",
    "    if current_position <= cliff_position:\n",
    "        return reward_cliff\n",
    "    if current_position >= end_position:\n",
    "        return reward_goal_state\n",
    "    return 0\n",
    "\n",
    "def is_terminating(current_position) -> bool:\n",
    "    if current_position <= cliff_position:\n",
    "        return True\n",
    "    if current_position >= end_position:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "In this simple environment, let us define an agent with a simple random strategy. On every step, the agent randomly decides to go left or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy() -> int:\n",
    "    if np.random.random() >= 0.5:\n",
    "        return 1 # Right\n",
    "    else:\n",
    "        return -1 # Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Average reward: [0.00, 0.00, nan, nan, nan, nan]\n",
      "[1000] Average reward: [0.00, 1.04, 2.09, 2.93, 3.83, 5.00]\n",
      "[2000] Average reward: [0.00, 0.96, 1.90, 2.77, 3.76, 5.00]\n",
      "[3000] Average reward: [0.00, 0.95, 1.87, 2.80, 3.82, 5.00]\n",
      "[4000] Average reward: [0.00, 0.95, 1.90, 2.84, 3.88, 5.00]\n",
      "[5000] Average reward: [0.00, 1.00, 1.98, 2.90, 3.89, 5.00]\n",
      "[6000] Average reward: [0.00, 1.01, 2.00, 2.95, 3.93, 5.00]\n",
      "[7000] Average reward: [0.00, 1.00, 1.99, 2.95, 3.95, 5.00]\n",
      "[8000] Average reward: [0.00, 1.01, 2.00, 2.95, 3.96, 5.00]\n",
      "[9000] Average reward: [0.00, 1.00, 1.99, 2.94, 3.94, 5.00]\n",
      "[10000] Average reward: [0.00, 0.99, 1.97, 2.92, 3.92, 5.00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-b19b0ba98f43>:31: RuntimeWarning: invalid value encountered in true_divide\n",
      "  expected_return = ', '.join(f'{q:.2f}' for q in value_sum / n_hits)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Global buffers to perform averaging later\n",
    "value_sum = np.zeros(end_position + 1)\n",
    "n_hits = np.zeros(end_position + 1)\n",
    "\n",
    "n_iter = 10001\n",
    "for i in range(n_iter):\n",
    "    position_history = [] # A log of positions in this episode\n",
    "    current_position = starting_position # Reset\n",
    "    while True:\n",
    "        # Append position to log\n",
    "        position_history.append(current_position)\n",
    "\n",
    "        if is_terminating(current_position):\n",
    "            break\n",
    "        \n",
    "        # Update current position according to strategy\n",
    "        current_position += strategy()\n",
    "\n",
    "    # Now the episode has finished, what was the reward?\n",
    "    current_reward = reward(current_position)\n",
    "    \n",
    "    # Now add the reward to the buffers that allow you to calculate the average\n",
    "    for pos in position_history:\n",
    "        value_sum[pos] += current_reward\n",
    "        n_hits[pos] += 1\n",
    "        \n",
    "    # Now calculate the average for this episode and print\n",
    "    expected_return = ', '.join(f'{q:.2f}' for q in value_sum / n_hits)\n",
    "    if i%1000==0:\n",
    "        print(\"[{}] Average reward: [{}]\".format(i, expected_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
